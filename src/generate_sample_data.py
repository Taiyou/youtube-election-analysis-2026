"""
ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
APIã‚­ãƒ¼ãŒãªãã¦ã‚‚å¯è¦–åŒ–ã®ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
"""
import random
from datetime import datetime, timedelta

import pandas as pd

from config import (
    DATA_DIR,
    PREFECTURE_DISTRICTS,
    REGIONAL_PARTY_STRENGTH,
    PREFECTURE_REGION_TYPE,
    PR_BLOCK_PREFECTURES,
)

random.seed(42)

PARTIES = ["è‡ªç”±æ°‘ä¸»å…š", "æ—¥æœ¬ç¶­æ–°ã®ä¼š", "ç«‹æ†²æ°‘ä¸»å…š", "å›½æ°‘æ°‘ä¸»å…š", "æ—¥æœ¬å…±ç”£å…š", "ã‚Œã„ã‚æ–°é¸çµ„", "å‚æ”¿å…š", "ãƒãƒ¼ãƒ ã¿ã‚‰ã„"]
ISSUES = ["æ¶ˆè²»ç¨ãƒ»ç‰©ä¾¡é«˜", "å®‰å…¨ä¿éšœ", "ç§»æ°‘ãƒ»å¤–å›½äºº", "çµŒæ¸ˆæ”¿ç­–", "ç¤¾ä¼šä¿éšœ", "æ”¿æ²»æ”¹é©", "ãã®ä»–"]

SAMPLE_TITLES = [
    "ã€è¡†é™¢é¸2026ã€‘å„å…šã®æ¶ˆè²»ç¨æ”¿ç­–ã‚’å¾¹åº•æ¯”è¼ƒ",
    "é«˜å¸‚é¦–ç›¸ãŒèªã‚‹çµŒæ¸ˆæ”¿ç­–ã®ãƒ“ã‚¸ãƒ§ãƒ³",
    "ä¸­é“æ”¹é©é€£åˆ é‡ç”°ä»£è¡¨ã®è¡—é ­æ¼”èª¬",
    "è¡†è­°é™¢é¸æŒ™ äº‰ç‚¹ã¾ã¨ã‚ 2026å¹´2æœˆ",
    "æ¶ˆè²»ç¨ã‚¼ãƒ­ã¯å®Ÿç¾å¯èƒ½ã‹ï¼Ÿå°‚é–€å®¶ãŒåˆ†æ",
    "è‡ªæ°‘ãƒ»ç¶­æ–°é€£ç«‹ã®è¡Œæ–¹ã¨é¸æŒ™æˆ¦ç•¥",
    "ç‰©ä¾¡é«˜å¯¾ç­– å„å…šã®å…¬ç´„ã‚’æ¯”è¼ƒã—ã¦ã¿ãŸ",
    "å°æ¹¾æœ‰äº‹ã¨æ—¥æœ¬ã®å®‰å…¨ä¿éšœ è¡†é™¢é¸ã®äº‰ç‚¹",
    "å¤–å›½äººæ”¿ç­– å„å…šã®ã‚¹ã‚¿ãƒ³ã‚¹ã¯ï¼Ÿ",
    "ã€ç·Šæ€¥è§£èª¬ã€‘å›½ä¼šå†’é ­è§£æ•£ã®çœŸæ„ã¨ã¯",
    "è‹¥è€…ãŒèªã‚‹ ä»Šå›ã®é¸æŒ™ã§å¤§äº‹ãªã“ã¨",
    "é¸æŒ™åŒºæƒ…å‹¢ æ¿€æˆ¦åŒºã‚’å¾¹åº•è§£èª¬",
    "è³ƒä¸Šã’æ”¿ç­– ã©ã®å…šãŒä¸€ç•ªç¾å®Ÿçš„ï¼Ÿ",
    "ç¤¾ä¼šä¿éšœã®æœªæ¥ å¹´é‡‘ãƒ»åŒ»ç™‚ã¯ã©ã†ãªã‚‹",
    "æ”¿æ²»ã¨ã‚«ãƒå•é¡Œ æœ‰æ¨©è€…ã®å£°ã¯",
    "æ¯”ä¾‹ä»£è¡¨ å„å…šã®è­°å¸­äºˆæ¸¬",
    "åˆã‚ã¦ã®é¸æŒ™ æŠ•ç¥¨ã®ä»•æ–¹ã‚¬ã‚¤ãƒ‰",
    "å¤§é›ªã§æŠ•ç¥¨ç‡ã«å½±éŸ¿ï¼ŸçœŸå†¬é¸æŒ™ã®èª²é¡Œ",
    "é–‹ç¥¨é€Ÿå ±ã«å‘ã‘ã¦æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆè§£èª¬",
    "ã‚Œã„ã‚æ–°é¸çµ„ å±±æœ¬å¤ªéƒã®è¨´ãˆ",
    "ãƒãƒ¼ãƒ ã¿ã‚‰ã„å®‰é‡ä»£è¡¨ AIã§å¤‰ãˆã‚‹æ”¿æ²»ã®æœªæ¥",
    "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã§æ”¿æ²»ã¯å¤‰ã‚ã‚‹ã‹ï¼Ÿãƒãƒ¼ãƒ ã¿ã‚‰ã„ã®æŒ‘æˆ¦",
]


def generate_video_details():
    """å‹•ç”»è©³ç´°ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿"""
    rows = []
    base_date = datetime(2026, 1, 1)

    for i in range(200):
        pub_date = base_date + timedelta(
            days=random.randint(0, 38),
            hours=random.randint(0, 23),
            minutes=random.randint(0, 59),
        )

        # é¸æŒ™å…¬ç¤ºå¾Œï¼ˆ1/27ä»¥é™ï¼‰ã¯æŠ•ç¨¿æ•°ãŒå¢—åŠ 
        days_from_start = (pub_date - base_date).days
        view_multiplier = 1.0 + max(0, (days_from_start - 26)) * 0.3

        views = int(random.lognormvariate(10, 1.5) * view_multiplier)
        likes = int(views * random.uniform(0.01, 0.08))
        comments = int(views * random.uniform(0.002, 0.02))

        party = random.choice(PARTIES + ["å€‹äºº"] * 5)
        title = random.choice(SAMPLE_TITLES)
        if party != "å€‹äºº":
            title = f"ã€{party}ã€‘{title}"

        # æ”¿å…šå‹•ç”»ã¯æ”¿å…šãƒãƒ£ãƒ³ãƒãƒ«IDã‚’ä½¿ç”¨ï¼ˆanalyze_channelsã¨ã®æ•´åˆæ€§ï¼‰
        if party != "å€‹äºº":
            channel_id = f"ch_{party}"
            channel_title = f"{party}å…¬å¼ãƒãƒ£ãƒ³ãƒãƒ«"
        else:
            channel_id = f"ch_{i % 50:03d}"
            channel_title = f"ãƒãƒ£ãƒ³ãƒãƒ«{i % 50}"

        rows.append({
            "video_id": f"sample_{i:04d}",
            "title": title,
            "channel_id": channel_id,
            "channel_title": channel_title,
            "published_at": pub_date.isoformat() + "Z",
            "tags": [],
            "category_id": "25",
            "duration": f"PT{random.randint(3, 120)}M{random.randint(0,59)}S",
            "view_count": views,
            "like_count": likes,
            "comment_count": comments,
        })

    return pd.DataFrame(rows)


def generate_comments():
    """ã‚³ãƒ¡ãƒ³ãƒˆã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæ‹¡å¼µãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼‰"""
    positive_templates = [
        "ã“ã®æ”¿ç­–ã«æœŸå¾…ã—ã¦ã„ã¾ã™", "å¿œæ´ã—ã¦ã„ã¾ã™ï¼", "ç´ æ™´ã‚‰ã—ã„æ¼”èª¬ã§ã—ãŸ",
        "ã¨ã¦ã‚‚åˆ†ã‹ã‚Šã‚„ã™ã„è§£èª¬", "æŠ•ç¥¨ã®å‚è€ƒã«ãªã‚Šã¾ã—ãŸ",
        "å…·ä½“çš„ã§ç¾å®Ÿçš„ãªæ”¿ç­–ã ã¨æ€ã„ã¾ã™", "ã“ã®å€™è£œè€…ã«ä¸€ç¥¨å…¥ã‚Œã¾ã™",
        "é ‘å¼µã£ã¦ãã ã•ã„ï¼ä¿¡é ¼ã—ã¦ã„ã¾ã™", "ã•ã™ãŒã€èª¬å¾—åŠ›ãŒã‚ã‚Šã¾ã™ã­",
        "è³›æˆã§ã™ã€‚ãœã²å®Ÿç¾ã—ã¦ã»ã—ã„", "ã‚„ã£ã±ã‚Šã“ã®æ”¿å…šãŒå®‰å¿ƒã§ãã‚‹",
        "æ„Ÿå‹•ã—ã¾ã—ãŸã€‚æ—¥æœ¬ã®æœªæ¥ã«å¸Œæœ›ãŒæŒã¦ã¾ã™",
        "å…±æ„Ÿã—ã¾ã™ã€‚ã‚‚ã£ã¨å¤šãã®äººã«è¦‹ã¦ã»ã—ã„",
        "ã“ã†ã„ã†æ”¿æ²»å®¶ã‚’å¾…ã£ã¦ã„ã¾ã—ãŸ", "æœ¬å½“ã«ç´ æ™´ã‚‰ã—ã„æ”¿ç­–æè¨€",
        "ã“ã®äººãªã‚‰ä»»ã›ã‚‰ã‚Œã‚‹", "çš„ç¢ºãªåˆ†æã§å‹‰å¼·ã«ãªã‚Šã¾ã—ãŸ",
        "æœŸå¾…é€šã‚Šã®å†…å®¹ã§ã—ãŸã€‚æ”¯æŒã—ã¾ã™",
    ]
    negative_templates = [
        "ã“ã®æ”¿ç­–ã«ã¯åå¯¾ã§ã™", "ä¿¡ç”¨ã§ããªã„", "ã‚‚ã£ã¨å…·ä½“çš„ãªæ”¿ç­–ã‚’",
        "å›½æ°‘ã‚’ãƒã‚«ã«ã—ã¦ã„ã‚‹", "å¤±æœ›ã—ã¾ã—ãŸ",
        "å£ã ã‘ã§ä½•ã‚‚å¤‰ã‚ã‚‰ãªã„", "ç¨é‡‘ã®ç„¡é§„é£ã„",
        "ã“ã‚“ãªæ”¿ç­–ã§ã¯æ—¥æœ¬ã¯ãƒ€ãƒ¡ã«ãªã‚‹", "å˜˜ã°ã‹ã‚Šã§ä¿¡ç”¨ã§ããªã„",
        "ç„¡è²¬ä»»ãªç™ºè¨€ã ã¨æ€ã„ã¾ã™", "ã‚‚ã†è¾ã‚ã¦ã»ã—ã„",
        "ã“ã®æ”¿å…šã«ã¯æŠ•ç¥¨ã—ã¾ã›ã‚“", "çŸ›ç›¾ã ã‚‰ã‘ã®å…¬ç´„ã§ã™ã­",
        "éç¾å®Ÿçš„ã™ãã‚‹ã€‚å®Ÿç¾ä¸å¯èƒ½", "æœ‰æ¨©è€…ã‚’é¦¬é¹¿ã«ã—ãŸæ”¿ç­–",
        "è£é‡‘å•é¡Œã®èª¬æ˜ãŒã¾ã è¶³ã‚Šãªã„", "å¢—ç¨ã°ã‹ã‚Šã§ç”Ÿæ´»ãŒè‹¦ã—ã„",
        "æœ€æ‚ªã®æ”¿ç­–ã€‚æ’¤å›ã™ã¹ã",
    ]
    neutral_templates = [
        "ä»–ã®å…šã®æ”¿ç­–ã‚‚çŸ¥ã‚ŠãŸã„", "æŠ•ç¥¨æ—¥ã¯2æœˆ8æ—¥ã§ã™ã­", "æƒ…å ±ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™",
        "ã‚‚ã†å°‘ã—è©³ã—ãèããŸã„", "é¸æŒ™åŒºã¯ã©ã“ã§ã™ã‹ï¼Ÿ",
        "å„å…šã®æ¯”è¼ƒãŒã‚ã‚‹ã¨åŠ©ã‹ã‚Šã¾ã™", "å®¢è¦³çš„ãªãƒ‡ãƒ¼ã‚¿ã§åˆ¤æ–­ã—ãŸã„",
        "ã©ã®æ”¿å…šã‚‚ä¸€é•·ä¸€çŸ­ã§ã™ã­", "åˆã‚ã¦é¸æŒ™ã«è¡Œãã¾ã™",
        "äº‰ç‚¹ãŒå¤šã™ãã¦è¿·ã£ã¦ã„ã¾ã™", "æœŸæ—¥å‰æŠ•ç¥¨ã¯ä½•æ—¥ã‹ã‚‰ã§ã™ã‹",
        "ã“ã®å•é¡Œã«ã¤ã„ã¦ã‚‚ã£ã¨è­°è«–ã—ã¦ã»ã—ã„",
        "çµå±€ã©ã®å…šãŒã„ã„ã®ã‹åˆ†ã‹ã‚‰ãªã„", "å„å€™è£œè€…ã®å®Ÿç¸¾ã‚’æ•™ãˆã¦ãã ã•ã„",
        "è‹¥ã„ä¸–ä»£ã®å£°ã‚‚èã„ã¦ã»ã—ã„", "é›ªã®å½±éŸ¿ãŒæ°—ã«ãªã‚Šã¾ã™",
        "ã“ã®å‹•ç”»ã‚·ãƒªãƒ¼ã‚ºã¯å‚è€ƒã«ãªã‚‹", "æ¬¡ã®å‹•ç”»ã‚‚æ¥½ã—ã¿ã«ã—ã¦ã„ã¾ã™",
    ]

    # ã‚³ãƒ¡ãƒ³ãƒˆã«ä»˜åŠ ã™ã‚‹ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé‡è¤‡å›é¿ç”¨ï¼‰
    suffixes = [
        "", "ã€‚", "ï¼", "ã­", "ã‚ˆ", "ãª", "ã‹ãª", "ã¨æ€ã†",
        "ã§ã™", "ã§ã™ã­", "â€¦", "ã€‚ã€‚ã€‚", "w", "ğŸ’ª", "ğŸ‘",
    ]

    rows = []
    for i in range(500):
        templates = random.choices(
            [positive_templates, negative_templates, neutral_templates],
            weights=[0.3, 0.3, 0.4],
        )[0]

        pub_date = datetime(2026, 1, 15) + timedelta(
            days=random.randint(0, 24),
            hours=random.randint(0, 23),
        )

        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ©ãƒ³ãƒ€ãƒ ãªã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä»˜åŠ ã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
        text = random.choice(templates) + random.choice(suffixes)

        # å…¨200å‹•ç”»ã«åˆ†æ•£ï¼ˆ0-19ã§ã¯ãªã0-199ï¼‰
        rows.append({
            "video_id": f"sample_{random.randint(0, 199):04d}",
            "comment_id": f"comment_{i:05d}",
            "author": f"ãƒ¦ãƒ¼ã‚¶ãƒ¼{i}",
            "text": text,
            "like_count": random.randint(0, 200),
            "published_at": pub_date.isoformat() + "Z",
        })

    return pd.DataFrame(rows)


def generate_channel_stats():
    """ãƒãƒ£ãƒ³ãƒãƒ«çµ±è¨ˆã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿"""
    data = {
        "è‡ªç”±æ°‘ä¸»å…š": {"subscribers": 280000, "videos": 3200, "views": 150000000},
        "æ—¥æœ¬ç¶­æ–°ã®ä¼š": {"subscribers": 120000, "videos": 1800, "views": 60000000},
        "ç«‹æ†²æ°‘ä¸»å…š": {"subscribers": 95000, "videos": 2500, "views": 45000000},
        "å›½æ°‘æ°‘ä¸»å…š": {"subscribers": 180000, "videos": 1200, "views": 80000000},
        "æ—¥æœ¬å…±ç”£å…š": {"subscribers": 65000, "videos": 4500, "views": 35000000},
        "ã‚Œã„ã‚æ–°é¸çµ„": {"subscribers": 350000, "videos": 2800, "views": 200000000},
        "å‚æ”¿å…š": {"subscribers": 220000, "videos": 1500, "views": 110000000},
        "ãƒãƒ¼ãƒ ã¿ã‚‰ã„": {"subscribers": 63000, "videos": 450, "views": 4000000},
    }

    rows = []
    for party, stats in data.items():
        rows.append({
            "channel_id": f"ch_{party}",
            "channel_title": f"{party}å…¬å¼ãƒãƒ£ãƒ³ãƒãƒ«",
            "party_name": party,
            "subscriber_count": stats["subscribers"] + random.randint(-5000, 5000),
            "video_count": stats["videos"] + random.randint(-50, 50),
            "view_count": stats["views"] + random.randint(-1000000, 1000000),
            "collected_at": datetime.now().isoformat(),
        })

    return pd.DataFrame(rows)


def generate_media_channels():
    """ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ»YouTuberãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿"""
    media_data = {
        # === ãƒ†ãƒ¬ãƒ“ãƒ»å ±é“ãƒ¡ãƒ‡ã‚£ã‚¢ ===
        "ANNnewsCHï¼ˆãƒ†ãƒ¬ãƒ“æœæ—¥ï¼‰": {
            "category": "ãƒ†ãƒ¬ãƒ“å ±é“",
            "subscribers": 4000000, "videos": 85000, "views": 8500000000,
            "election_videos": 120, "election_views": 45000000,
        },
        "TBS NEWS DIG": {
            "category": "ãƒ†ãƒ¬ãƒ“å ±é“",
            "subscribers": 3160000, "videos": 62000, "views": 4570000000,
            "election_videos": 95, "election_views": 38000000,
        },
        "ãƒ†ãƒ¬æ±BIZ": {
            "category": "ãƒ†ãƒ¬ãƒ“å ±é“",
            "subscribers": 2300000, "videos": 28000, "views": 3200000000,
            "election_videos": 85, "election_views": 32000000,
        },
        "æ—¥ãƒ†ãƒ¬NEWS": {
            "category": "ãƒ†ãƒ¬ãƒ“å ±é“",
            "subscribers": 2150000, "videos": 55000, "views": 3800000000,
            "election_videos": 110, "election_views": 35000000,
        },
        "FNNãƒ—ãƒ©ã‚¤ãƒ ã‚ªãƒ³ãƒ©ã‚¤ãƒ³": {
            "category": "ãƒ†ãƒ¬ãƒ“å ±é“",
            "subscribers": 1200000, "videos": 40000, "views": 2100000000,
            "election_videos": 80, "election_views": 22000000,
        },
        "NHK": {
            "category": "ãƒ†ãƒ¬ãƒ“å ±é“",
            "subscribers": 4000000, "videos": 12000, "views": 3500000000,
            "election_videos": 60, "election_views": 28000000,
        },
        # === æ”¿æ²»ãƒ»çµŒæ¸ˆã‚³ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚¿ãƒ¼YouTuber ===
        "PIVOT": {
            "category": "ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒ‡ã‚£ã‚¢",
            "subscribers": 3700000, "videos": 3500, "views": 1800000000,
            "election_videos": 25, "election_views": 15000000,
        },
        "å €æ±Ÿè²´æ–‡ï¼ˆãƒ›ãƒªã‚¨ãƒ¢ãƒ³ï¼‰": {
            "category": "æ”¿æ²»ã‚³ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚¿ãƒ¼",
            "subscribers": 2180000, "videos": 8500, "views": 2200000000,
            "election_videos": 30, "election_views": 18000000,
        },
        "é«˜æ©‹æ´‹ä¸€ãƒãƒ£ãƒ³ãƒãƒ«": {
            "category": "æ”¿æ²»ã‚³ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚¿ãƒ¼",
            "subscribers": 1290000, "videos": 2800, "views": 751000000,
            "election_videos": 45, "election_views": 25000000,
        },
        "ReHacQ": {
            "category": "ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒ‡ã‚£ã‚¢",
            "subscribers": 1000000, "videos": 1200, "views": 450000000,
            "election_videos": 20, "election_views": 12000000,
        },
        "ç«¹ç”°æ’æ³°ãƒãƒ£ãƒ³ãƒãƒ«": {
            "category": "æ”¿æ²»ã‚³ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚¿ãƒ¼",
            "subscribers": 773000, "videos": 3200, "views": 520000000,
            "election_videos": 35, "election_views": 14000000,
        },
        "æ–‡åŒ–äººæ”¾é€å±€": {
            "category": "æ”¿æ²»ã‚³ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚¿ãƒ¼",
            "subscribers": 740000, "videos": 5500, "views": 480000000,
            "election_videos": 40, "election_views": 12000000,
        },
        "ä¸Šå¿µå¸ãƒãƒ£ãƒ³ãƒãƒ«": {
            "category": "æ”¿æ²»ã‚³ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚¿ãƒ¼",
            "subscribers": 644000, "videos": 2400, "views": 350000000,
            "election_videos": 38, "election_views": 10000000,
        },
        "ä¸€æœˆä¸‡å†Š": {
            "category": "æ”¿æ²»ã‚³ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚¿ãƒ¼",
            "subscribers": 480000, "videos": 6000, "views": 380000000,
            "election_videos": 28, "election_views": 8000000,
        },
        # === é¸æŒ™å°‚é–€ãƒ»ãã®ä»– ===
        "é¸æŒ™ãƒ‰ãƒƒãƒˆã‚³ãƒ ": {
            "category": "é¸æŒ™å°‚é–€ãƒ¡ãƒ‡ã‚£ã‚¢",
            "subscribers": 85000, "videos": 800, "views": 35000000,
            "election_videos": 55, "election_views": 9000000,
        },
    }

    rows = []
    for channel, stats in media_data.items():
        rows.append({
            "channel_name": channel,
            "category": stats["category"],
            "subscriber_count": stats["subscribers"] + random.randint(-10000, 10000),
            "total_video_count": stats["videos"] + random.randint(-100, 100),
            "total_view_count": stats["views"] + random.randint(-5000000, 5000000),
            "election_video_count": stats["election_videos"] + random.randint(-5, 5),
            "election_view_count": stats["election_views"] + random.randint(-500000, 500000),
            "avg_election_views": (stats["election_views"] // stats["election_videos"])
                                  + random.randint(-10000, 10000),
            "collected_at": datetime.now().isoformat(),
        })

    return pd.DataFrame(rows)


def generate_media_video_topics():
    """ãƒ¡ãƒ‡ã‚£ã‚¢å‹•ç”»ã®æ”¿å…šè¨€åŠãƒˆãƒ”ãƒƒã‚¯åˆ†æï¼ˆã©ã®æ”¿å…šãŒã©ã‚Œã ã‘å–ã‚Šä¸Šã’ã‚‰ã‚ŒãŸã‹ï¼‰"""
    parties = PARTIES + ["å…¬æ˜å…š"]
    # å„ãƒ¡ãƒ‡ã‚£ã‚¢ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰æ”¿å…šã¸ã®è¨€åŠå‰²åˆï¼ˆå†ç”Ÿå›æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
    mention_weights = {
        "è‡ªç”±æ°‘ä¸»å…š":   0.28,
        "æ—¥æœ¬ç¶­æ–°ã®ä¼š":  0.14,
        "ç«‹æ†²æ°‘ä¸»å…š":    0.16,
        "å›½æ°‘æ°‘ä¸»å…š":    0.10,
        "æ—¥æœ¬å…±ç”£å…š":    0.04,
        "ã‚Œã„ã‚æ–°é¸çµ„":  0.06,
        "å‚æ”¿å…š":        0.04,
        "ãƒãƒ¼ãƒ ã¿ã‚‰ã„":  0.08,
        "å…¬æ˜å…š":        0.06,
        "ãã®ä»–":        0.04,
    }

    total_third_party_views = 1800000000

    rows = []
    for party, weight in mention_weights.items():
        base_views = int(total_third_party_views * weight)
        rows.append({
            "party_name": party,
            "media_mention_views": base_views + random.randint(-5000000, 5000000),
            "media_mention_share": round(weight * 100, 1),
            "tv_media_views": int(base_views * 0.55) + random.randint(-2000000, 2000000),
            "youtuber_views": int(base_views * 0.30) + random.randint(-1000000, 1000000),
            "other_creator_views": int(base_views * 0.15) + random.randint(-500000, 500000),
        })

    return pd.DataFrame(rows)


def generate_news_articles():
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸»è¦ãƒ¡ãƒ‡ã‚£ã‚¢ã®é¸æŒ™å ±é“ï¼‰"""
    sources = {
        "NHK": {"type": "å…¬å…±æ”¾é€", "credibility": 4.5, "political_lean": 0.0},
        "æœæ—¥æ–°è": {"type": "å…¨å›½ç´™", "credibility": 4.0, "political_lean": -0.3},
        "èª­å£²æ–°è": {"type": "å…¨å›½ç´™", "credibility": 4.0, "political_lean": 0.2},
        "æ¯æ—¥æ–°è": {"type": "å…¨å›½ç´™", "credibility": 3.8, "political_lean": -0.2},
        "ç”£çµŒæ–°è": {"type": "å…¨å›½ç´™", "credibility": 3.5, "political_lean": 0.4},
        "æ—¥æœ¬çµŒæ¸ˆæ–°è": {"type": "çµŒæ¸ˆç´™", "credibility": 4.2, "political_lean": 0.1},
        "æ±äº¬æ–°è": {"type": "åœ°æ–¹ç´™", "credibility": 3.5, "political_lean": -0.4},
        "å…±åŒé€šä¿¡": {"type": "é€šä¿¡ç¤¾", "credibility": 4.0, "political_lean": 0.0},
        "æ™‚äº‹é€šä¿¡": {"type": "é€šä¿¡ç¤¾", "credibility": 4.0, "political_lean": 0.0},
        "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹": {"type": "ãƒãƒ¼ã‚¿ãƒ«", "credibility": 3.2, "political_lean": 0.0},
        "æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³": {"type": "çµŒæ¸ˆãƒ¡ãƒ‡ã‚£ã‚¢", "credibility": 3.8, "political_lean": 0.0},
        "ç¾ä»£ãƒ“ã‚¸ãƒã‚¹": {"type": "Webãƒ¡ãƒ‡ã‚£ã‚¢", "credibility": 3.3, "political_lean": -0.1},
        "æ–‡æ˜¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³": {"type": "Webãƒ¡ãƒ‡ã‚£ã‚¢", "credibility": 3.5, "political_lean": 0.0},
        "AERAdot.": {"type": "Webãƒ¡ãƒ‡ã‚£ã‚¢", "credibility": 3.3, "political_lean": -0.2},
        "NewsPicks": {"type": "çµŒæ¸ˆãƒ¡ãƒ‡ã‚£ã‚¢", "credibility": 3.5, "political_lean": 0.1},
    }

    article_topics = [
        "è¡†é™¢é¸æƒ…å‹¢èª¿æŸ»", "å„å…šã®å…¬ç´„æ¯”è¼ƒ", "æ¶ˆè²»ç¨æ”¿ç­–", "å®‰å…¨ä¿éšœæ”¿ç­–",
        "çµŒæ¸ˆå¯¾ç­–", "å€™è£œè€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", "é¸æŒ™åŒºæƒ…å‹¢åˆ†æ", "æŠ•ç¥¨ç‡äºˆæ¸¬",
        "ä¸–è«–èª¿æŸ»çµæœ", "å…šé¦–è¨è«–", "è¡—é ­æ¼”èª¬ãƒ«ãƒ", "æ”¿ç­–åˆ†æ",
        "é€£ç«‹æ”¿æ¨©ã®è¡Œæ–¹", "è‹¥è€…ã®æŠ•ç¥¨è¡Œå‹•", "SNSé¸æŒ™æˆ¦ç•¥", "AIæ”¿æ²»ã®å¯èƒ½æ€§",
        "ãƒãƒ¼ãƒ ã¿ã‚‰ã„ç‰¹é›†", "è£é‡‘å•é¡Œã¨æœ‰æ¨©è€…", "æ¯”ä¾‹ä»£è¡¨åˆ¶åº¦è§£èª¬", "æœŸæ—¥å‰æŠ•ç¥¨å‹•å‘",
    ]

    base_date = datetime(2026, 1, 1)
    rows = []
    for i in range(600):
        pub_date = base_date + timedelta(
            days=random.randint(0, 38),
            hours=random.randint(6, 23),
            minutes=random.randint(0, 59),
        )
        days_from_start = (pub_date - base_date).days

        # å…¬ç¤ºæ—¥ä»¥é™ã¯è¨˜äº‹æ•°å¢—åŠ 
        article_boost = 1.0 + max(0, (days_from_start - 26)) * 0.5

        source_name = random.choice(list(sources.keys()))
        source_info = sources[source_name]

        # æ”¿å…šã¸ã®è¨€åŠ
        mentioned_parties = []
        for party in PARTIES + ["å…¬æ˜å…š"]:
            base_prob = 0.15
            if party == "è‡ªç”±æ°‘ä¸»å…š":
                base_prob = 0.45
            elif party == "æ—¥æœ¬ç¶­æ–°ã®ä¼š":
                base_prob = 0.30
            elif party == "ç«‹æ†²æ°‘ä¸»å…š":
                base_prob = 0.28
            elif party == "ãƒãƒ¼ãƒ ã¿ã‚‰ã„":
                base_prob = 0.12
            if random.random() < base_prob:
                mentioned_parties.append(party)

        if not mentioned_parties:
            mentioned_parties = [random.choice(PARTIES)]

        tone = round(random.gauss(source_info["political_lean"], 0.3), 2)
        tone = max(-1, min(1, tone))

        base_pv = random.lognormvariate(9, 1.2)
        pv = int(base_pv * article_boost * (1 + source_info["credibility"] / 5))

        topic = random.choice(article_topics)

        rows.append({
            "article_id": f"news_{i:04d}",
            "source": source_name,
            "source_type": source_info["type"],
            "credibility_score": source_info["credibility"],
            "title": f"{topic}ï¼š{random.choice(mentioned_parties)}ã®{'å‹•å‘' if random.random() > 0.5 else 'æ”¿ç­–'}",
            "published_at": pub_date.isoformat(),
            "topic": topic,
            "mentioned_parties": "|".join(mentioned_parties),
            "tone": tone,
            "page_views": pv,
            "comment_count": int(pv * random.uniform(0.01, 0.05)),
            "share_count": int(pv * random.uniform(0.005, 0.03)),
        })

    return pd.DataFrame(rows)


def generate_news_polling():
    """ä¸–è«–èª¿æŸ»ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå¹³å‡å›å¸°ä»˜ããƒ‰ãƒªãƒ•ãƒˆãƒ¢ãƒ‡ãƒ«ï¼‰"""
    survey_sources = ["NHK", "æœæ—¥æ–°è", "èª­å£²æ–°è", "æ¯æ—¥æ–°è", "å…±åŒé€šä¿¡", "æ—¥æœ¬çµŒæ¸ˆæ–°è"]

    # æ”¯æŒç‡ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
    support_baseline = {
        "è‡ªç”±æ°‘ä¸»å…š": 32.0, "æ—¥æœ¬ç¶­æ–°ã®ä¼š": 12.0, "ç«‹æ†²æ°‘ä¸»å…š": 10.0,
        "å›½æ°‘æ°‘ä¸»å…š": 8.0, "æ—¥æœ¬å…±ç”£å…š": 3.5, "ã‚Œã„ã‚æ–°é¸çµ„": 4.0,
        "å‚æ”¿å…š": 2.5, "å…¬æ˜å…š": 4.0, "ãƒãƒ¼ãƒ ã¿ã‚‰ã„": 3.0, "æ”¯æŒãªã—": 21.0,
    }

    # å„æ”¿å…šã®é€±ã”ã¨ã®ç´¯ç©ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆå¹³å‡å›å¸°ä»˜ãï¼‰
    party_current = {party: rate for party, rate in support_baseline.items()}

    # å¹³å‡å›å¸°ã®å¼·ã•ï¼ˆ0ã«è¿‘ã„ã»ã©å¼·ã„å›å¸°ï¼‰
    mean_reversion_strength = 0.3

    rows = []
    base_date = datetime(2026, 1, 5)
    for week in range(6):
        survey_date = base_date + timedelta(weeks=week)
        for source in random.sample(survey_sources, k=random.randint(2, 4)):
            for party, base_rate in support_baseline.items():
                # OUéç¨‹é¢¨ã®å¹³å‡å›å¸°ãƒ‰ãƒªãƒ•ãƒˆ
                current = party_current[party]
                deviation = current - base_rate
                drift = -mean_reversion_strength * deviation + random.gauss(0, 0.6)
                party_current[party] = max(0.5, current + drift)

                # èª¿æŸ»æ©Ÿé–¢ã”ã¨ã®ãƒã‚¤ã‚¢ã‚¹ï¼ˆãƒã‚¦ã‚¹ã‚¨ãƒ•ã‚§ã‚¯ãƒˆï¼‰
                house_effect = random.gauss(0, 0.5)
                rate = max(0.5, party_current[party] + house_effect)

                rows.append({
                    "survey_date": survey_date.strftime("%Y-%m-%d"),
                    "source": source,
                    "party_name": party,
                    "support_rate": round(rate, 1),
                    "sample_size": random.randint(1000, 2500),
                })

    return pd.DataFrame(rows)


def generate_news_daily_coverage():
    """æ—¥åˆ¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹å ±é“é‡ãƒ‡ãƒ¼ã‚¿"""
    base_date = datetime(2026, 1, 1)
    rows = []
    for day_offset in range(39):
        date = base_date + timedelta(days=day_offset)
        base_articles = 15 + random.randint(-3, 3)
        if day_offset >= 26:
            base_articles = int(base_articles * (1.5 + (day_offset - 26) * 0.1))
        if day_offset >= 35:
            base_articles = int(base_articles * 1.8)

        rows.append({
            "date": date.strftime("%Y-%m-%d"),
            "article_count": base_articles,
            "total_page_views": base_articles * random.randint(8000, 25000),
            "avg_tone": round(random.gauss(0.0, 0.15), 3),
        })

    return pd.DataFrame(rows)


# === é¸æŒ™åŒºãƒ»å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ ===


# PREFECTURE_DISTRICTS, REGIONAL_PARTY_STRENGTH, PREFECTURE_REGION_TYPE,
# PR_BLOCK_PREFECTURES ã¯ config.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿

SURNAMES = [
    "ä½è—¤", "éˆ´æœ¨", "é«˜æ©‹", "ç”°ä¸­", "ä¼Šè—¤", "æ¸¡è¾º", "å±±æœ¬", "ä¸­æ‘", "å°æ—", "åŠ è—¤",
    "å‰ç”°", "å±±ç”°", "ä½ã€…æœ¨", "æ¾æœ¬", "äº•ä¸Š", "æœ¨æ‘", "æ—", "æ–è—¤", "æ¸…æ°´", "å±±å´",
    "æ£®", "æ± ç”°", "æ©‹æœ¬", "é˜¿éƒ¨", "çŸ³å·", "å±±ä¸‹", "ä¸­å³¶", "å‰ç”°", "è—¤ç”°", "å°å·",
    "å²¡ç”°", "å¾Œè—¤", "é•·è°·å·", "çŸ³äº•", "æ‘ä¸Š", "è¿‘è—¤", "å‚æœ¬", "é è—¤", "é’æœ¨", "è—¤äº•",
    "è¥¿æ‘", "ç¦ç”°", "å¤ªç”°", "ä¸‰æµ¦", "å²¡æœ¬", "æ¾ç”°", "ä¸­é‡", "åŸç”°", "å°é‡", "ç”°æ‘",
    "ç«¹å†…", "é‡‘å­", "å’Œç”°", "ä¸­å±±", "çŸ³ç”°", "ä¸Šç”°", "æ£®ç”°", "åŸ", "æŸ´ç”°", "é…’äº•",
    "å·¥è—¤", "æ¨ªå±±", "å®®å´", "å®®æœ¬", "å†…ç”°", "é«˜æœ¨", "å®‰è—¤", "è°·å£", "å¤§é‡", "ä¸¸å±±",
]
GIVEN_NAMES_M = [
    "å¤ªéƒ", "ä¸€éƒ", "å¥ä¸€", "èª ", "æµ©", "éš†", "ä¿®", "å‰›", "åš", "æ­£",
    "è±Š", "é€²", "å‹‡", "ç¿”", "å¤§è¼”", "æ‹“ä¹Ÿ", "é›„ä¸€", "ç›´æ¨¹", "å’Œä¹Ÿ", "å“²ä¹Ÿ",
    "ç§€æ¨¹", "é›…å½¦", "ç¾©æ˜", "ä¿¡äºŒ", "æ•å¤«", "å¹¸å¤«", "æ­£ç¾©", "æ…ä¸€", "å…‰ç”·", "è‹±æ˜",
]
GIVEN_NAMES_F = [
    "èŠ±å­", "ç¾å’²", "é™½å­", "è£•å­", "çœŸç†å­", "ç”±ç¾å­", "æµå­", "å¹¸å­", "æ˜ç¾", "å’Œå­",
    "äº¬å­", "ä¹…ç¾å­", "æ™ºå­", "æ´‹å­", "ç¯€å­", "åƒæµå­", "ç›´ç¾", "éº»è¡£", "å½©", "ç¾ç©‚",
]


def generate_district_candidates():
    """289å°é¸æŒ™åŒºã®å€™è£œè€…ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    all_parties = list(REGIONAL_PARTY_STRENGTH["rural_ldp"].keys())

    rows = []
    for pref_code, (pref_name, n_districts) in PREFECTURE_DISTRICTS.items():
        region_type = PREFECTURE_REGION_TYPE[pref_code]
        party_probs = REGIONAL_PARTY_STRENGTH[region_type]

        for dist_num in range(1, n_districts + 1):
            district_name = f"{pref_name.replace('çœŒ','').replace('åºœ','').replace('éƒ½','').replace('é“','')}{dist_num}åŒº"
            if pref_name == "åŒ—æµ·é“":
                district_name = f"åŒ—æµ·é“{dist_num}åŒº"
            elif pref_name == "æ±äº¬éƒ½":
                district_name = f"æ±äº¬{dist_num}åŒº"

            n_candidates = random.randint(2, 4)

            parties_pool = list(party_probs.keys())
            weights = [party_probs[p] for p in parties_pool]
            chosen_parties = []
            for _ in range(n_candidates):
                if not parties_pool:
                    break
                w_sum = sum(weights)
                normalized = [w / w_sum for w in weights]
                idx = random.choices(range(len(parties_pool)), weights=normalized, k=1)[0]
                chosen_parties.append(parties_pool[idx])
                parties_pool.pop(idx)
                weights.pop(idx)

            vote_shares = sorted(
                [random.uniform(0.15, 0.50) for _ in range(n_candidates)],
                reverse=True,
            )
            total_share = random.uniform(0.85, 0.95)
            raw_sum = sum(vote_shares)
            vote_shares = [v / raw_sum * total_share for v in vote_shares]

            winner_share = vote_shares[0]
            margin = winner_share - vote_shares[1] if len(vote_shares) > 1 else winner_share

            for rank, (party, share) in enumerate(zip(chosen_parties, vote_shares), 1):
                is_male = random.random() > 0.25
                surname = random.choice(SURNAMES)
                given = random.choice(GIVEN_NAMES_M if is_male else GIVEN_NAMES_F)
                name = f"{surname} {given}"

                rows.append({
                    "prefecture_code": pref_code,
                    "prefecture_name": pref_name,
                    "district_number": dist_num,
                    "district_name": district_name,
                    "candidate_name": name,
                    "party": party,
                    "age": random.randint(32, 75),
                    "is_incumbent": random.random() < (0.6 if rank == 1 else 0.2),
                    "predicted_vote_share": round(share, 4),
                    "predicted_rank": rank,
                    "margin": round(margin if rank == 1 else round(winner_share - share, 4), 4),
                    "youtube_score": round(random.uniform(0.1, 1.0), 3),
                    "news_mentions": random.randint(5, 120),
                })

    return pd.DataFrame(rows)


def generate_prefecture_summary():
    """éƒ½é“åºœçœŒåˆ¥ã®é¸æŒ™äºˆæ¸¬é›†ç´„ãƒ‡ãƒ¼ã‚¿"""
    rows = []
    for pref_code, (pref_name, n_districts) in PREFECTURE_DISTRICTS.items():
        region_type = PREFECTURE_REGION_TYPE[pref_code]
        party_probs = REGIONAL_PARTY_STRENGTH[region_type]

        party_seats = {}
        remaining = n_districts
        sorted_parties = sorted(party_probs.items(), key=lambda x: -x[1])
        for i, (party, prob) in enumerate(sorted_parties):
            if i == len(sorted_parties) - 1:
                seats = remaining
            else:
                seats = min(remaining, round(n_districts * prob + random.gauss(0, 0.5)))
                seats = max(0, seats)
            party_seats[party] = seats
            remaining -= seats
            if remaining <= 0:
                break

        total = sum(party_seats.values())
        if total != n_districts:
            dominant = max(party_seats, key=party_seats.get)
            party_seats[dominant] += (n_districts - total)

        dominant_party = max(party_seats, key=party_seats.get)

        block_name = ""
        for block, prefs in PR_BLOCK_PREFECTURES.items():
            if pref_code in prefs:
                block_name = block
                break

        rows.append({
            "prefecture_code": pref_code,
            "prefecture_name": pref_name,
            "region_block": block_name,
            "total_smd_seats": n_districts,
            "dominant_party": dominant_party,
            "è‡ªç”±æ°‘ä¸»å…š": party_seats.get("è‡ªç”±æ°‘ä¸»å…š", 0),
            "ç«‹æ†²æ°‘ä¸»å…š": party_seats.get("ç«‹æ†²æ°‘ä¸»å…š", 0),
            "æ—¥æœ¬ç¶­æ–°ã®ä¼š": party_seats.get("æ—¥æœ¬ç¶­æ–°ã®ä¼š", 0),
            "å›½æ°‘æ°‘ä¸»å…š": party_seats.get("å›½æ°‘æ°‘ä¸»å…š", 0),
            "å…¬æ˜å…š": party_seats.get("å…¬æ˜å…š", 0),
            "æ—¥æœ¬å…±ç”£å…š": party_seats.get("æ—¥æœ¬å…±ç”£å…š", 0),
            "ã‚Œã„ã‚æ–°é¸çµ„": party_seats.get("ã‚Œã„ã‚æ–°é¸çµ„", 0),
            "å‚æ”¿å…š": party_seats.get("å‚æ”¿å…š", 0),
            "ãƒãƒ¼ãƒ ã¿ã‚‰ã„": party_seats.get("ãƒãƒ¼ãƒ ã¿ã‚‰ã„", 0),
            "ç„¡æ‰€å±": party_seats.get("ç„¡æ‰€å±", 0),
            "battleground_count": random.randint(0, max(1, n_districts // 3)),
        })

    return pd.DataFrame(rows)


def generate_all_sample_data():
    """å…¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    print("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")

    raw_dir = DATA_DIR / "raw"
    raw_dir.mkdir(parents=True, exist_ok=True)

    timestamp = "sample"

    df_details = generate_video_details()
    df_details.to_csv(
        raw_dir / f"video_details_{timestamp}.csv", index=False, encoding="utf-8-sig"
    )

    df_comments = generate_comments()
    df_comments.to_csv(
        raw_dir / f"comments_{timestamp}.csv", index=False, encoding="utf-8-sig"
    )

    df_channels = generate_channel_stats()
    df_channels.to_csv(
        raw_dir / f"channel_stats_{timestamp}.csv", index=False, encoding="utf-8-sig"
    )

    # processed ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚‚ä½œæˆ
    processed_dir = DATA_DIR / "processed"
    processed_dir.mkdir(parents=True, exist_ok=True)

    # æ—¥åˆ¥æŠ•ç¨¿æ•°
    df_details["published_at"] = pd.to_datetime(df_details["published_at"])
    df_details["date"] = df_details["published_at"].dt.date
    daily_counts = df_details.groupby("date").size().reset_index(name="video_count")
    daily_counts.to_csv(processed_dir / "daily_video_counts.csv", index=False)

    # æ—¥åˆ¥å†ç”Ÿå›æ•°
    daily_views = df_details.groupby("date")["view_count"].sum().reset_index()
    daily_views.to_csv(processed_dir / "daily_views.csv", index=False)

    # äº‰ç‚¹åˆ¥çµ±è¨ˆ
    issue_data = []
    for issue in ISSUES:
        n = random.randint(10, 50)
        views = random.randint(50000, 2000000)
        issue_data.append({
            "issue": issue,
            "video_count": n,
            "total_views": views,
            "avg_views": views // n,
            "total_likes": int(views * 0.03),
            "total_comments": int(views * 0.005),
        })
    issue_stats = pd.DataFrame(issue_data).sort_values("total_views", ascending=False)
    issue_stats.to_csv(
        processed_dir / "issue_stats.csv", index=False, encoding="utf-8-sig"
    )

    # ãƒãƒ£ãƒ³ãƒãƒ«åˆ†æ
    df_channels.to_csv(
        processed_dir / "channel_analysis.csv", index=False, encoding="utf-8-sig"
    )

    # æ”¿å…šå‹•ç”»çµ±è¨ˆ
    party_video_data = []
    for party in PARTIES:
        n = random.randint(5, 30)
        views = random.randint(30000, 500000)
        party_video_data.append({
            "party_name": party,
            "video_count": n,
            "total_views": views,
            "avg_views": views // n,
            "total_likes": int(views * 0.04),
        })
    pd.DataFrame(party_video_data).to_csv(
        processed_dir / "party_video_stats.csv", index=False, encoding="utf-8-sig"
    )

    # ãƒ¡ãƒ‡ã‚£ã‚¢ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿
    df_media = generate_media_channels()
    df_media.to_csv(
        raw_dir / f"media_channels_{timestamp}.csv", index=False, encoding="utf-8-sig"
    )
    df_media.to_csv(
        processed_dir / "media_channels.csv", index=False, encoding="utf-8-sig"
    )

    # ãƒ¡ãƒ‡ã‚£ã‚¢æ”¿å…šè¨€åŠåˆ†æ
    df_media_topics = generate_media_video_topics()
    df_media_topics.to_csv(
        processed_dir / "media_party_mentions.csv", index=False, encoding="utf-8-sig"
    )

    # æ„Ÿæƒ…åˆ†æ
    sentiment_data = pd.DataFrame([
        {"sentiment": "positive", "count": 148},
        {"sentiment": "neutral", "count": 210},
        {"sentiment": "negative", "count": 142},
    ])
    sentiment_data.to_csv(processed_dir / "sentiment_counts.csv", index=False)

    # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ‡ãƒ¼ã‚¿
    df_news = generate_news_articles()
    df_news.to_csv(
        processed_dir / "news_articles.csv", index=False, encoding="utf-8-sig"
    )

    # ä¸–è«–èª¿æŸ»ãƒ‡ãƒ¼ã‚¿
    df_polling = generate_news_polling()
    df_polling.to_csv(
        processed_dir / "news_polling.csv", index=False, encoding="utf-8-sig"
    )

    # æ—¥åˆ¥å ±é“é‡
    df_daily_news = generate_news_daily_coverage()
    df_daily_news.to_csv(
        processed_dir / "news_daily_coverage.csv", index=False, encoding="utf-8-sig"
    )

    # é¸æŒ™åŒºãƒ»å€™è£œè€…ãƒ‡ãƒ¼ã‚¿
    df_districts = generate_district_candidates()
    df_districts.to_csv(
        processed_dir / "district_candidates.csv", index=False, encoding="utf-8-sig"
    )

    df_pref_summary = generate_prefecture_summary()
    df_pref_summary.to_csv(
        processed_dir / "prefecture_summary.csv", index=False, encoding="utf-8-sig"
    )

    print(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†!")
    print(f"  raw: {raw_dir}")
    print(f"  processed: {processed_dir}")


if __name__ == "__main__":
    generate_all_sample_data()
